# metadata specialised for each experiment
core:
  version: 0.0.1
  tags:
    - connectomics

eval_only: True

model:
  _target_: src.pl_modules.model.MyModel
  name: UNet3D
  weights: /cifs/data/tserre/CLPS_Serre_Lab/projects/prj_connectomics/contlearn/checkpoints/epoch=51-step=3327.ckpt
  in_channels: 2
  out_channels: 1

data:
  datamodule:
    _target_: src.pl_data.datamodule.MyDataModule
    val_percentage: 0.0
    use_train_dataset: Celltype_Prediction
    use_val_dataset: Celltype_Prediction
    datasets:
      Celltype_Prediction:
        train:
          _target_: src.pl_data.dataset.Volumetric
          train: True
          path: gs://serrelab/connectomics/npy/celltype/cells_0.npz
          # path: gs://serrelab/connectomics/tfrecords/celltype/cell_type_10_64_15.tfrecords_train.tfrecords
          # len: 97
          len: 128
          shape: [16, 160, 160]
          trim_dims: False
        val:
          _target_: src.pl_data.dataset.VolumetricTEST
          train: False
          path: /cifs/data/tserre/CLPS_Serre_Lab/projects/prj_connectomics/connectomics_data_scratch_v1/mag1_membranes/x0010/y0079/z0001/110629_k0725_mag1_x0010_y0079_z0001.raw.npy
          trim_dims: [[0, 32], [0, 384], [0, 384]]
          # path: gs://serrelab/connectomics/tfrecords/celltype/cell_type_10_64_15.tfrecords_val.tfrecords
          # len: 11
          len: 1
          shape: [384, 1152, 1152]
          # trim_dims: False
        test:
          _target_: src.pl_data.dataset.VolumetricTEST
          train: False
          path: /cifs/data/tserre/CLPS_Serre_Lab/projects/prj_connectomics/connectomics_data_scratch_v1/mag1_membranes/x0010/y0079/z0001/110629_k0725_mag1_x0010_y0079_z0001.raw.npy
          trim_dims: [[0, 32], [0, 32], [0, 32]]
          # path: gs://serrelab/connectomics/tfrecords/celltype/cell_type_10_64_15.tfrecords_val.tfrecords
          # len: 11
          len: 1
          shape: [384, 1152, 1152]
          # trim_dims: False

    num_workers:
      train: 4
      val: 4
      test: 4

    batch_size:
      train: 1  # 240  # 128
      val: 1 # 240  # 128
      test: 1  # 128
hydra:
  run:
    dir: results/${now:%Y-%m-%d}/${now:%H-%M-%S}
    # dir: gs://serrelab/connectomics/results/${now:%Y-%m-%d}/${now:%H-%M-%S}

  sweep:
    dir: results/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}/
    # dir: gs://serrelab/connectomics/results/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}/
    subdir: ${hydra.job.num}_${hydra.job.id}

  job:
    env_set:
      WANDB_START_METHOD: thread

logging:
  n_elements_to_log: 4  # Set to 16 if tpu=1
  log_every_n_steps: 10

  # log frequency
  val_check_interval: 1.
  # progress_bar_refresh_rate: 20

  wandb:
    project: cont-learn
    entity: serrelab

    watch:
      log: 'all'
      log_freq: 10

  lr_monitor:
    logging_interval: "step"
    log_momentum: False

loss:
  # _target_: src.pl_modules.losses.dice_loss  # cce
  _target_: src.pl_modules.losses.no_loss
  weights: 4.76

metric:
  name: Accuracy
  _target_: torchmetrics.Accuracy  # src.pl_modules.losses.no_loss
  is_function: True

optim:
  optimizer:
    #  Adam-oriented deep learning
    _target_: torch.optim.Adam
    #  These are all default parameters for the Adam optimizer
    lr: 1e-4  # 0.001
    betas: [ 0.9, 0.999 ]
    eps: 1e-08
    # weight_decay: 1e-7

  use_lr_scheduler: True
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    T_0: 10
    T_mult: 2
    eta_min: 0 # min value for the lr
    last_epoch: -1

train:
  # reproducibility
  deterministic: False
  random_seed: 42

  # training

  pl_trainer:
    fast_dev_run: False # Enable this for debug purposes
    gpus: 1
    # tpu_cores: 1
    precision: 16
    max_steps: 60000  # 30000
    # accumulate_grad_batches: 1
    num_sanity_val_steps: 2
    gradient_clip_val: 10000000.0  # 10.

  monitor_metric: 'val_loss'
  monitor_metric_mode: 'min'

  early_stopping:
    patience: 42
    verbose: False

  model_checkpoints:
    save_top_k: 1
    verbose: False

